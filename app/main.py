"""
FastAPI MVP: Chat + Plan generation (24h/72h) + RAG integration (Chroma + LlamaIndex)
Auth: HTTP Basic
Logs: Python logging
LLM: vLLM-hosted Mistral (safetensors, v0.2)
Docs store: ./docs (txt/md)
RAG vectorstore: ./vectorstore/chroma
Conversation Memory: LangChain ConversationBufferMemory

Run:
uvicorn main:app --host 0.0.0.0 --port 8000 --reload
"""

import os
import csv
import uuid
import datetime
import logging
import requests
from pathlib import Path
from collections import defaultdict

from fastapi import FastAPI, Depends, HTTPException, UploadFile, File
from fastapi.security import HTTPBasic, HTTPBasicCredentials
from fastapi.responses import FileResponse
from pydantic import BaseModel
from reportlab.lib.pagesizes import A4
from reportlab.pdfgen import canvas
from dotenv import load_dotenv

from llama_index.vector_stores.chroma import ChromaVectorStore
from llama_index.core import StorageContext, VectorStoreIndex
from llama_index.embeddings.huggingface import HuggingFaceEmbedding
from chromadb import PersistentClient

# üß† LangChain memory
from langchain.memory import ConversationBufferMemory
from langchain.schema import HumanMessage, AIMessage

# -----------------
# Load env vars
# -----------------
load_dotenv()
PERSIST_DIR = "vectorstore/chroma"
COLLECTION_NAME = "island_docs"

DOCS_DIR = Path('./docs')
EXPORT_DIR = Path('./exports')
DOCS_DIR.mkdir(exist_ok=True)
EXPORT_DIR.mkdir(exist_ok=True)

API_USER = os.getenv('MVP_USER', 'admin')
API_PASS = os.getenv('MVP_PASS', 'password')

# vLLM API endpoint
VLLM_API_URL = os.getenv("VLLM_API_URL", "http://localhost:8001/v1/completions")
MODEL_NAME = os.getenv("VLLM_MODEL_NAME", "/mnt/c/Users/sarah/Desktop/TheLab_/models/TinyLlama-1.1B-Chat-v1.0")

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger('mvp')

app = FastAPI(title='MVP Crisis Chat & Plan (RAG + vLLM)')
security = HTTPBasic()
ACTION_LOGS = []

# -----------------
# Initialize RAG
# -----------------
logger.info("‚öôÔ∏è Initializing RAG components...")
os.environ["TRANSFORMERS_OFFLINE"] = "1"

sbert_model_path = "../models/all-MiniLM-L6-v2"
embed_model = HuggingFaceEmbedding(
    model_name=sbert_model_path,
    model_kwargs={"device": "cuda" if os.system("nvidia-smi > /dev/null 2>&1") == 0 else "cpu"}
)

chroma_client = PersistentClient(path=PERSIST_DIR)
collection = chroma_client.get_or_create_collection(COLLECTION_NAME)
vector_store = ChromaVectorStore(chroma_collection=collection)
storage_context = StorageContext.from_defaults(vector_store=vector_store)

index = VectorStoreIndex.from_vector_store(
    vector_store, storage_context=storage_context, embed_model=embed_model
)
query_engine = index.as_retriever(similarity_top_k=3)

MAX_CONTEXT_TOKENS = 1000

def query_knowledge_base(question: str) -> str:
    """Retrieve context from vectorstore."""
    retrieved_nodes = query_engine.retrieve(question)
    context = "\n".join([n.text for n in retrieved_nodes])
    logger.info(f"üìö Retrieved {len(retrieved_nodes)} context chunks from RAG.")
    return context or "No relevant information found."

# -----------------
# Conversation memory
# -----------------
USER_MEMORIES = defaultdict(lambda: ConversationBufferMemory(return_messages=True))

# -----------------
# Helper: vLLM generator
# -----------------
def generate_with_vllm(prompt, max_tokens=1500, temperature=0.7):
    """Send prompt to vLLM /v1/completions endpoint safely."""
    # Truncate or simplify prompt if too long
    if len(prompt) > 100:
        prompt = prompt[:100] + "\n[... truncated for model input ...]"

    full_prompt = (
            "Tu es un assistant IA de gestion de crise et de r√©silience territoriale. "
            "R√©ponds de mani√®re concise, structur√©e et sans r√©p√©ter la question.\n\n"
            f"{prompt.strip()}\n\n"
            "R√©ponse :"
    )

    payload = {
        "model": MODEL_NAME,
        "prompt": full_prompt,
        "max_tokens": max_tokens,
        "temperature": temperature
    }

    print(payload)
    r = requests.post("http://localhost:8001/v1/completions", json=payload)
    r.raise_for_status()
    return r.json()["choices"][0]["text"]




# -----------------
# Pydantic model
# -----------------
class ChatRequest(BaseModel):
    question: str

# -----------------
# Auth helper
# -----------------
def verify_credentials(credentials: HTTPBasicCredentials = Depends(security)):
    if credentials.username != API_USER or credentials.password != API_PASS:
        raise HTTPException(status_code=401, detail="Unauthorized")
    return credentials.username

# -----------------
# Chat endpoint
# -----------------
@app.post("/chat")
def chat(request: ChatRequest, username: str = Depends(verify_credentials)):
    input_text = request.question.strip()
    memory = USER_MEMORIES[username]
    rag_context = query_knowledge_base(input_text)

    history_text = "\n".join([
        f"{msg.type.capitalize()}: {msg.content}"
        for msg in memory.chat_memory.messages[-5:]
    ])

    # Full prompt (exactly as your version)
    prompt = f"""
    Tu es un **assistant IA de gestion de crise et de r√©silience territoriale**, charg√© d‚Äôaider des d√©cideurs locaux √† **prioriser et planifier** des projets de r√©silience √† partir de donn√©es structur√©es (tabulaires) et de retours d‚Äôexp√©rience (retex).

    Knowledge Base Context:
    {rag_context}

    Conversation History:
    {history_text}

    User: {input_text}

    ---

    ## üéØ OBJECTIFS PRINCIPAUX
    1. Identifier et **prioriser** les projets de r√©silience les plus pertinents pour le territoire.
    2. Fournir pour chaque projet une **fiche d√©cisionnelle compl√®te** :
    - Description courte et objectifs
    - Justification issue du contexte RAG
    - √âvaluation de faisabilit√©, impact, co√ªt, et urgence
    - Ressources et parties prenantes cl√©s
    - Calendrier pr√©visionnel (30 / 90 / 180 jours)
    - Risques et mesures d‚Äôatt√©nuation
    - Indicateurs de suivi (KPIs)
    3. Produire une **synth√®se claire** et un **bloc JSON exploitable** pour automatiser la planification.

    ---

    ## üìä CRIT√àRES DE PRIORISATION
    Chaque projet est √©valu√© selon 6 crit√®res pond√©r√©s : # V√©rifier KPIS ou alors script pour KPIS (agent) √† d√©terminer

    | Crit√®re | Description | Note (0‚Äì1) | Poids (w) |
    |----------|-------------|------------|------------|
    | E (Urgence) | Probabilit√© d‚Äôoccurrence √† court terme | 0‚Äì1 | 0.25 |
    | I (Impact) | Population / infrastructures concern√©es | 0‚Äì1 | 0.30 |
    | C (Co√ªt) | Niveau de ressources n√©cessaires | 0‚Äì1 | 0.05 |
    | F (Faisabilit√©) | Capacit√© technique, politique, humaine | 0‚Äì1 | 0.20 |
    | L (Effet de levier) | Co-b√©n√©fices / synergies | 0‚Äì1 | 0.15 |
    | T (Temporalit√©) | D√©lai d‚Äôobtention de b√©n√©fices | 0‚Äì1 | 0.05 |

    **Score de priorit√©** :
    Score_priorit√© = normalize( wE*(1-E) + wI*I + wF*F + wL*L + wT*T - wC*C )

    ---

    ## üìã FORMAT DE SORTIE ATTENDU

    ### üßæ 1. R√©sum√© pour d√©cideur
    - Trois phrases maximum, r√©sumant les priorit√©s et recommandations principales.

    ### üß± 2. Liste des projets (Top N)
    Pour chaque projet :
    - `id`
    - `titre_court`
    - `description_br√®ve`
    - `score_priorite` (0‚Äì100)
    - `confiance` (haute / moyenne / faible)
    - `justification_synthese` (r√©sum√© + sources RAG)
    - `sources`
    - `ressources_estimees` (budget, personnel, mat√©riel)
    - `calendrier_recommand√©` (30j / 90j / 180j)
    - `parties_prenantes`
    - `principaux_risques` + `mesures_dattenuation`
    - `kpis` (‚â•3)
    - `actions_imm√©diates`
    - `score_components` (valeurs des 6 crit√®res + score final)

    ### üíª 3. Bloc JSON machine-lisible
    {{
    "meta": {{
        "generated_at": "datetime.datetime.now().isoformat()",
        "rag_ids_used": ["..."],
        "history_hash": "<hash>"
    }},
    "summary": "...",
    "projects": [
        {{
        "id": "proj_001",
        "title": "Protection des r√©seaux d‚Äôeau potable",
        "priority_rank": 1,
        "priority_score": 92.3,
        "confidence": "haute",
        "justification": "Bas√© sur 3 retex post-Irma indiquant panne r√©seau >48h.",
        "sources": ["RAG_doc_12", "table_infra_2017"],
        "resources_estimate": {{"budget_eur": 150000, "fte": 3, "equipment": ["pompes", "g√©n√©rateurs"]}},
        "timeline": {{"30d": ["s√©curiser 2 stations"], "90d": ["renforcer conduites"], "180d": ["auditer r√©seau complet"]}},
        "stakeholders": ["Commune", "ARS", "Protection civile"],
        "risks": [{{"risk": "retard d‚Äôapprovisionnement", "mitigation": "pr√©voir stock tampon"}}],
        "kpis": [{{"kpi": "% foyers raccord√©s", "target": ">95%", "measure": "mensuel"}}],
        "immediate_actions": ["contracter fournisseurs", "v√©rifier stock g√©n√©rateurs"],
        "score_components": {{"E": 0.8, "I": 0.9, "C": 0.3, "F": 0.7, "L": 0.4, "T": 0.8, "computed_score": 92.3}}
        }}
    ],
    "data_gaps": ["manque donn√©es co√ªt maintenance r√©seau"],
    "next_steps": ["v√©rifier inventaire mat√©riel", "collecter donn√©es actualis√©es sur capacit√©s locales"]
    }}

    ---

    ## ‚öôÔ∏è R√àGLES DE CITATION ET TRA√áABILIT√â
    - Toute donn√©e chiffr√©e ou factuelle doit mentionner sa **source RAG** (ex: `RAG_table_infra[row=12]`).
    - Si une donn√©e est estim√©e, marque-la comme `ESTIMATION` et explique la m√©thode utilis√©e.
    - Si des sources sont contradictoires, indique la version la plus probable et propose un test ou une collecte compl√©mentaire.

    ---

    ## üß≠ GESTION DES CONTRAINTES ET CONTEXTE
    - Respecte strictement les contraintes budg√©taires, temporelles ou g√©ographiques mentionn√©es par l‚Äôutilisateur.
    - Si aucune contrainte n‚Äôest donn√©e, propose 5 projets prioritaires par d√©faut.
    - Mentionne les donn√©es manquantes et propose des actions concr√®tes pour les combler.
    - Ne produis jamais d‚Äôactions ill√©gales, irr√©alistes ou contraires √† l‚Äô√©thique.

    ---

    ## ‚úçÔ∏è STYLE DE SORTIE
    - Ton professionnel, clair et concis.
    - D‚Äôabord la **r√©ponse actionnable**, ensuite les **d√©tails**.
    - √âvite le jargon technique non expliqu√©.
    - Mentionne la **confiance** de chaque recommandation (haute / moyenne / faible).

    Assistant:
    """

    # üîπ Generate via vLLM
    answer = generate_with_vllm(prompt)

    # üîπ Update memory
    memory.chat_memory.add_user_message(input_text)
    memory.chat_memory.add_ai_message(answer)

    # üîπ Log
    ACTION_LOGS.append({
        'time': datetime.datetime.now().isoformat(),
        'username': username,
        'question': input_text,
        'context_used': rag_context[:1000],
        'answer': answer
    })

    return {
        "answer": answer,
        "context_used": rag_context,
        "conversation_turns": len(memory.chat_memory.messages) // 2
    }

# -----------------
# Reset + History Endpoints
# -----------------
@app.delete("/chat/reset")
def reset_chat(username: str = Depends(verify_credentials)):
    USER_MEMORIES[username].clear()
    return {"message": f"Conversation reset for {username}."}

@app.get("/chat/history")
def get_history(username: str = Depends(verify_credentials)):
    memory = USER_MEMORIES[username]
    return [{"role": msg.type, "content": msg.content} for msg in memory.chat_memory.messages]

# -----------------
# Plan generation
# -----------------
@app.get("/plan")
def plan(horizon: int = 24, username: str = Depends(verify_credentials)):
    if horizon == 24:
        plan_text = (
            "Plan 24h:\n"
            "- Abri / Shelter\n"
            "- Eau et nourriture\n"
            "- Recherche des personnes disparues\n"
            "- Soins m√©dicaux urgents\n"
            "- Logistique et transport\n"
            "- √ânergie et √©lectricit√©\n"
            "- Premiers soutiens psychologiques\n"
        )
    elif horizon == 72:
        plan_text = (
            "Plan 72h:\n"
            "- Reconstitution des syst√®mes vitaux (eau, √©lectricit√©)\n"
            "- Coordination ONG / Gouvernement\n"
            "- D√©blaiement et s√©curisation\n"
            "- Soutien psychologique\n"
            "- Remise en place des infrastructures critiques\n"
            "- Appels d‚Äôoffre et reconstruction\n"
        )
    else:
        plan_text = "Plan non disponible pour cet horizon"

    filename = EXPORT_DIR / f"plan_{horizon}h_{uuid.uuid4().hex[:6]}.pdf"
    c = canvas.Canvas(str(filename), pagesize=A4)
    for i, line in enumerate(plan_text.split("\n")):
        c.drawString(50, 800 - i * 20, line)
    c.save()

    ACTION_LOGS.append({
        'time': datetime.datetime.now().isoformat(),
        'plan_horizon': horizon,
        'file': str(filename)
    })

    return FileResponse(str(filename), media_type='application/pdf', filename=f"plan_{horizon}h.pdf")

# -----------------
# Upload document
# -----------------
@app.post("/upload_doc")
def upload_doc(file: UploadFile = File(...), username: str = Depends(verify_credentials)):
    filepath = DOCS_DIR / file.filename
    with open(filepath, 'wb') as f:
        f.write(file.file.read())
    return {"message": f"Document {file.filename} uploaded successfully."}

# -----------------
# Logs
# -----------------
@app.get("/logs")
def get_logs(username: str = Depends(verify_credentials)):
    return ACTION_LOGS

@app.get("/export_logs_csv")
def export_logs_csv(username: str = Depends(verify_credentials)):
    filename = EXPORT_DIR / f"logs_{uuid.uuid4().hex[:6]}.csv"
    with open(filename, 'w', newline='', encoding='utf-8') as csvfile:
        writer = csv.DictWriter(csvfile, fieldnames=['time', 'username', 'question', 'context_used', 'answer', 'plan_horizon', 'file'])
        writer.writeheader()
        for log in ACTION_LOGS:
            writer.writerow({k: log.get(k, "") for k in ['time', 'username', 'question', 'context_used', 'answer', 'plan_horizon', 'file']})
    return FileResponse(str(filename), media_type='text/csv', filename=filename.name)

# -----------------
# üß© MCP tool trigger endpoint
# -----------------
import requests

MCP_URL = "http://localhost:8100"

@app.post("/tool")
def call_mcp_tool(tool_name: str, params: dict, username: str = Depends(verify_credentials)):
    """
    Call a specific tool hosted on the MCP server (e.g. osm_data_collector or climate_forecast_collector).
    """
    try:
        response = requests.post(f"{MCP_URL}/tools/{tool_name}", json=params)
        response.raise_for_status()
        return response.json()
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Tool call failed: {e}")