"""
FastAPI MVP: Chat + Plan generation (24h/72h) + RAG integration (Chroma + LlamaIndex)
Auth: HTTP Basic
Logs: Python logging
LLM: Local Mistral (safetensors, v0.2)
Docs store: ./docs (txt/md)
RAG vectorstore: ./vectorstore/chroma
Conversation Memory: LangChain ConversationBufferMemory

Run:
uvicorn main:app --host 0.0.0.0 --port 8000 --reload
"""

import os
import csv
import uuid
import datetime
import logging
from pathlib import Path
from collections import defaultdict

from fastapi import FastAPI, Depends, HTTPException, UploadFile, File
from fastapi.security import HTTPBasic, HTTPBasicCredentials
from fastapi.responses import FileResponse
from pydantic import BaseModel
from reportlab.lib.pagesizes import A4
from reportlab.pdfgen import canvas
from dotenv import load_dotenv

import torch
from transformers import AutoTokenizer, AutoModelForCausalLM

from llama_index.vector_stores.chroma import ChromaVectorStore
from llama_index.core import StorageContext, VectorStoreIndex
from llama_index.embeddings.huggingface import HuggingFaceEmbedding
from chromadb import PersistentClient

# üß† LangChain memory
from langchain.memory import ConversationBufferMemory
from langchain.schema import HumanMessage, AIMessage

# -----------------
# Load env vars
# -----------------
load_dotenv()
MISTRAL_MODEL_DIR = os.getenv('MISTRAL_MODEL_DIR', '../models/Mistral-7B-Instruct-v0.2')
PERSIST_DIR = "vectorstore/chroma"
COLLECTION_NAME = "island_docs"

DOCS_DIR = Path('./docs')
EXPORT_DIR = Path('./exports')
DOCS_DIR.mkdir(exist_ok=True)
EXPORT_DIR.mkdir(exist_ok=True)

API_USER = os.getenv('MVP_USER', 'admin')
API_PASS = os.getenv('MVP_PASS', 'password')

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger('mvp')

app = FastAPI(title='MVP Crisis Chat & Plan (RAG-enabled)')
security = HTTPBasic()
ACTION_LOGS = []

# -----------------
# Load local Mistral model
# -----------------
device = 'cuda' if torch.cuda.is_available() else 'cpu'
logger.info(f"üß† Loading Mistral model from {MISTRAL_MODEL_DIR} on {device}...")

tokenizer = AutoTokenizer.from_pretrained(MISTRAL_MODEL_DIR, local_files_only=True, use_fast=False)
model = AutoModelForCausalLM.from_pretrained(
    MISTRAL_MODEL_DIR,
    local_files_only=True,
    torch_dtype=torch.float16 if device == 'cuda' else torch.float32,
    device_map='auto' if device == 'cuda' else None,
    trust_remote_code=True,
    low_cpu_mem_usage=True
)
logger.info("‚úÖ Mistral model loaded successfully.")

# -----------------
# Pydantic model
# -----------------
class ChatRequest(BaseModel):
    question: str

# -----------------
# Auth helper
# -----------------
def verify_credentials(credentials: HTTPBasicCredentials = Depends(security)):
    if credentials.username != API_USER or credentials.password != API_PASS:
        raise HTTPException(status_code=401, detail="Unauthorized")
    return credentials.username

# -----------------
# Initialize RAG
# -----------------
logger.info("‚öôÔ∏è Initializing RAG components...")
os.environ["TRANSFORMERS_OFFLINE"] = "1"

# Wrap SBERT in HuggingFaceEmbedding (compatible with LlamaIndex)
sbert_model_path = "../models/all-MiniLM-L6-v2"
embed_model = HuggingFaceEmbedding(
    model_name=sbert_model_path,
    model_kwargs={"device": "cuda" if torch.cuda.is_available() else "cpu"}
)

# Persistent Chroma vectorstore
chroma_client = PersistentClient(path=PERSIST_DIR)
collection = chroma_client.get_or_create_collection(COLLECTION_NAME)
vector_store = ChromaVectorStore(chroma_collection=collection)
storage_context = StorageContext.from_defaults(vector_store=vector_store)

# Initialize index
index = VectorStoreIndex.from_vector_store(vector_store, storage_context=storage_context, embed_model=embed_model)
query_engine = index.as_retriever(similarity_top_k=3)

MAX_CONTEXT_TOKENS = 1000

def query_knowledge_base(question: str) -> str:
    retrieved_nodes = query_engine.retrieve(question)
    context = "\n".join([n.text for n in retrieved_nodes])
    inputs = tokenizer(context, return_tensors="pt")
    if inputs.input_ids.size(1) > MAX_CONTEXT_TOKENS:
        context_tokens = inputs.input_ids[:, -MAX_CONTEXT_TOKENS:]
        context = tokenizer.decode(context_tokens[0], skip_special_tokens=True)
    logger.info(f"üìö Retrieved {len(retrieved_nodes)} context chunks from RAG.")
    return context or "No relevant information found."

# -----------------
# Conversation memory
# -----------------
USER_MEMORIES = defaultdict(lambda: ConversationBufferMemory(return_messages=True))

# -----------------
# Chat endpoint with conversation memory
# -----------------
@app.post("/chat")
def chat(request: ChatRequest, username: str = Depends(verify_credentials)):
    input_text = request.question.strip()

    # Retrieve user-specific memory
    memory = USER_MEMORIES[username]

    # Retrieve RAG context
    rag_context = query_knowledge_base(input_text)

    # Build conversation history (last 5 messages)
    history_text = "\n".join([
        f"{msg.type.capitalize()}: {msg.content}"
        for msg in memory.chat_memory.messages[-5:]
    ])

    prompt = f"""
Tu es un **assistant IA de gestion de crise et de r√©silience territoriale**, charg√© d‚Äôaider des d√©cideurs locaux √† **prioriser et planifier** des projets de r√©silience √† partir de donn√©es structur√©es (tabulaires) et de retours d‚Äôexp√©rience (retex).

Knowledge Base Context:
{rag_context}

Conversation History:
{history_text}

User: {input_text}

---

## üéØ OBJECTIFS PRINCIPAUX
1. Identifier et **prioriser** les projets de r√©silience les plus pertinents pour le territoire.
2. Fournir pour chaque projet une **fiche d√©cisionnelle compl√®te** :
   - Description courte et objectifs
   - Justification issue du contexte RAG
   - √âvaluation de faisabilit√©, impact, co√ªt, et urgence
   - Ressources et parties prenantes cl√©s
   - Calendrier pr√©visionnel (30 / 90 / 180 jours)
   - Risques et mesures d‚Äôatt√©nuation
   - Indicateurs de suivi (KPIs)
3. Produire une **synth√®se claire** et un **bloc JSON exploitable** pour automatiser la planification.

---

## üìä CRIT√àRES DE PRIORISATION
Chaque projet est √©valu√© selon 6 crit√®res pond√©r√©s :

| Crit√®re | Description | Note (0‚Äì1) | Poids (w) |
|----------|-------------|------------|------------|
| E (Urgence) | Probabilit√© d‚Äôoccurrence √† court terme | 0‚Äì1 | 0.25 |
| I (Impact) | Population / infrastructures concern√©es | 0‚Äì1 | 0.30 |
| C (Co√ªt) | Niveau de ressources n√©cessaires | 0‚Äì1 | 0.05 |
| F (Faisabilit√©) | Capacit√© technique, politique, humaine | 0‚Äì1 | 0.20 |
| L (Effet de levier) | Co-b√©n√©fices / synergies | 0‚Äì1 | 0.15 |
| T (Temporalit√©) | D√©lai d‚Äôobtention de b√©n√©fices | 0‚Äì1 | 0.05 |

**Score de priorit√©** :
Score_priorit√© = normalize( wE*(1-E) + wI*I + wF*F + wL*L + wT*T - wC*C )

---

## üìã FORMAT DE SORTIE ATTENDU

### üßæ 1. R√©sum√© pour d√©cideur
- Trois phrases maximum, r√©sumant les priorit√©s et recommandations principales.

### üß± 2. Liste des projets (Top N)
Pour chaque projet :
- `id`
- `titre_court`
- `description_br√®ve`
- `score_priorite` (0‚Äì100)
- `confiance` (haute / moyenne / faible)
- `justification_synthese` (r√©sum√© + sources RAG)
- `sources`
- `ressources_estimees` (budget, personnel, mat√©riel)
- `calendrier_recommand√©` (30j / 90j / 180j)
- `parties_prenantes`
- `principaux_risques` + `mesures_dattenuation`
- `kpis` (‚â•3)
- `actions_imm√©diates`
- `score_components` (valeurs des 6 crit√®res + score final)

### üíª 3. Bloc JSON machine-lisible
{
  "meta": {
    "generated_at": "<ISO8601>",
    "rag_ids_used": ["..."],
    "history_hash": "<hash>"
  },
  "summary": "...",
  "projects": [
    {
      "id": "proj_001",
      "title": "Protection des r√©seaux d‚Äôeau potable",
      "priority_rank": 1,
      "priority_score": 92.3,
      "confidence": "haute",
      "justification": "Bas√© sur 3 retex post-Irma indiquant panne r√©seau >48h.",
      "sources": ["RAG_doc_12", "table_infra_2017"],
      "resources_estimate": {"budget_eur": 150000, "fte": 3, "equipment": ["pompes", "g√©n√©rateurs"]},
      "timeline": {"30d": ["s√©curiser 2 stations"], "90d": ["renforcer conduites"], "180d": ["auditer r√©seau complet"]},
      "stakeholders": ["Commune", "ARS", "Protection civile"],
      "risks": [{"risk": "retard d‚Äôapprovisionnement", "mitigation": "pr√©voir stock tampon"}],
      "kpis": [{"kpi": "% foyers raccord√©s", "target": ">95%", "measure": "mensuel"}],
      "immediate_actions": ["contracter fournisseurs", "v√©rifier stock g√©n√©rateurs"],
      "score_components": {"E": 0.8, "I": 0.9, "C": 0.3, "F": 0.7, "L": 0.4, "T": 0.8, "computed_score": 92.3}
    }
  ],
  "data_gaps": ["manque donn√©es co√ªt maintenance r√©seau"],
  "next_steps": ["v√©rifier inventaire mat√©riel", "collecter donn√©es actualis√©es sur capacit√©s locales"]
}

---

## ‚öôÔ∏è R√àGLES DE CITATION ET TRA√áABILIT√â
- Toute donn√©e chiffr√©e ou factuelle doit mentionner sa **source RAG** (ex: `RAG_table_infra[row=12]`).
- Si une donn√©e est estim√©e, marque-la comme `ESTIMATION` et explique la m√©thode utilis√©e.
- Si des sources sont contradictoires, indique la version la plus probable et propose un test ou une collecte compl√©mentaire.

---

## üß≠ GESTION DES CONTRAINTES ET CONTEXTE
- Respecte strictement les contraintes budg√©taires, temporelles ou g√©ographiques mentionn√©es par l‚Äôutilisateur.
- Si aucune contrainte n‚Äôest donn√©e, propose 5 projets prioritaires par d√©faut.
- Mentionne les donn√©es manquantes et propose des actions concr√®tes pour les combler.
- Ne produis jamais d‚Äôactions ill√©gales, irr√©alistes ou contraires √† l‚Äô√©thique.

---

## ‚úçÔ∏è STYLE DE SORTIE
- Ton professionnel, clair et concis.
- D‚Äôabord la **r√©ponse actionnable**, ensuite les **d√©tails**.
- √âvite le jargon technique non expliqu√©.
- Mentionne la **confiance** de chaque recommandation (haute / moyenne / faible).

Assistant:
"""


    # Generate response with Mistral
    inputs = tokenizer(prompt, return_tensors="pt").to(device)
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=256,
            temperature=0.7,
            do_sample=True,
            pad_token_id=tokenizer.eos_token_id,
        )
    answer = tokenizer.decode(outputs[0], skip_special_tokens=True)

    # Update memory
    memory.chat_memory.add_user_message(input_text)
    memory.chat_memory.add_ai_message(answer)

    # Log
    ACTION_LOGS.append({
        'time': datetime.datetime.now().isoformat(),
        'username': username,
        'question': input_text,
        'context_used': rag_context[:1000],
        'answer': answer
    })

    return {
        "answer": answer,
        "context_used": rag_context,
        "conversation_turns": len(memory.chat_memory.messages) // 2
    }

# -----------------
# Reset + History Endpoints
# -----------------
@app.delete("/chat/reset")
def reset_chat(username: str = Depends(verify_credentials)):
    USER_MEMORIES[username].clear()
    return {"message": f"Conversation reset for {username}."}

@app.get("/chat/history")
def get_history(username: str = Depends(verify_credentials)):
    memory = USER_MEMORIES[username]
    return [
        {"role": msg.type, "content": msg.content}
        for msg in memory.chat_memory.messages
    ]

# -----------------
# Plan generation endpoint
# -----------------
@app.get("/plan")
def plan(horizon: int = 24, username: str = Depends(verify_credentials)):
    if horizon == 24:
        plan_text = (
            "Plan 24h:\n"
            "- Abri / Shelter\n"
            "- Eau et nourriture\n"
            "- Recherche des personnes disparues\n"
            "- Soins m√©dicaux urgents\n"
            "- Logistique et transport\n"
            "- √ânergie et √©lectricit√©\n"
            "- Premiers soutiens psychologiques\n"
        )
    elif horizon == 72:
        plan_text = (
            "Plan 72h:\n"
            "- Reconstitution des syst√®mes vitaux (eau, √©lectricit√©)\n"
            "- Coordination ONG / Gouvernement\n"
            "- D√©blaiement et s√©curisation\n"
            "- Soutien psychologique\n"
            "- Remise en place des infrastructures critiques\n"
            "- Appels d‚Äôoffre et reconstruction\n"
        )
    else:
        plan_text = "Plan non disponible pour cet horizon"

    filename = EXPORT_DIR / f"plan_{horizon}h_{uuid.uuid4().hex[:6]}.pdf"
    c = canvas.Canvas(str(filename), pagesize=A4)
    for i, line in enumerate(plan_text.split("\n")):
        c.drawString(50, 800 - i * 20, line)
    c.save()

    ACTION_LOGS.append({
        'time': datetime.datetime.now().isoformat(),
        'plan_horizon': horizon,
        'file': str(filename)
    })

    return FileResponse(str(filename), media_type='application/pdf', filename=f"plan_{horizon}h.pdf")

# -----------------
# Upload new document
# -----------------
@app.post("/upload_doc")
def upload_doc(file: UploadFile = File(...), username: str = Depends(verify_credentials)):
    filepath = DOCS_DIR / file.filename
    with open(filepath, 'wb') as f:
        f.write(file.file.read())
    return {"message": f"Document {file.filename} uploaded successfully."}

# -----------------
# Logs endpoints
# -----------------
@app.get("/logs")
def get_logs(username: str = Depends(verify_credentials)):
    return ACTION_LOGS

@app.get("/export_logs_csv")
def export_logs_csv(username: str = Depends(verify_credentials)):
    filename = EXPORT_DIR / f"logs_{uuid.uuid4().hex[:6]}.csv"
    with open(filename, 'w', newline='', encoding='utf-8') as csvfile:
        writer = csv.DictWriter(csvfile, fieldnames=['time', 'username', 'question', 'context_used', 'answer', 'plan_horizon', 'file'])
        writer.writeheader()
        for log in ACTION_LOGS:
            writer.writerow({k: log.get(k, "") for k in ['time', 'username', 'question', 'context_used', 'answer', 'plan_horizon', 'file']})
    return FileResponse(str(filename), media_type='text/csv', filename=filename.name)
